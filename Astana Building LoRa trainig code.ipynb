{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.lora-dreambooth)\n",
        "\n",
        "# CSCI 585: Final Project<br><small><small>A Colab Notebook For Astana Building LoRA Training (Dreambooth Method)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPgBR3KM6E-Z"
      },
      "source": [
        "Adapted to Google Colab based on [kohya-ss/sd-script](https://github.com/kohya-ss/sd-scripts)<br>\n",
        "Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)<br>\n",
        "You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6E01tRBfQU"
      },
      "source": [
        "| Notebook Name | Description | Link |\n",
        "| --- | --- | --- |\n",
        "| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) |\n",
        "| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) |\n",
        "| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) |\n",
        "| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) |\n",
        "| Kohya Textual Inversion  | Textual Inversion Training | SOON |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. Install Kohya Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u3q60di584x",
        "outputId": "52cbee92-2a84-423f-dd2d-93f16801a7f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Dec  2 09:34:20 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Stored 'root_dir' (str)\n",
            "Stored 'repo_dir' (str)\n",
            "Stored 'tools_dir' (str)\n",
            "Stored 'finetune_dir' (str)\n",
            "Stored 'training_dir' (str)\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 2500, done.\u001b[K\n",
            "remote: Counting objects: 100% (2500/2500), done.\u001b[K\n",
            "remote: Compressing objects: 100% (933/933), done.\u001b[K\n",
            "remote: Total 2500 (delta 1682), reused 2230 (delta 1565), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2500/2500), 4.81 MiB | 19.95 MiB/s, done.\n",
            "Resolving deltas: 100% (1682/1682), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "root_dir = \"/content\"\n",
        "%store root_dir\n",
        "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
        "%store repo_dir\n",
        "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
        "%store tools_dir\n",
        "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
        "%store finetune_dir\n",
        "training_dir = str(root_dir)+\"/dreambooth\"\n",
        "%store training_dir\n",
        "\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "\n",
        "def clone_repo():\n",
        "  if os.path.isdir(repo_dir):\n",
        "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "  else:\n",
        "    %cd {root_dir}\n",
        "    !git clone {repo_url} {repo_dir}\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  clone_repo()\n",
        "\n",
        "%cd {root_dir}\n",
        "os.makedirs(repo_dir, exist_ok=True)\n",
        "os.makedirs(tools_dir, exist_ok=True)\n",
        "os.makedirs(finetune_dir, exist_ok=True)\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "\n",
        "if branch:\n",
        "  %cd {repo_dir}\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLja5dz7vQTM",
        "outputId": "52fa9c1f-9952-4395-c077-f4f3c9263146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kaleido-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install kaleido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNn0g1pnHfk5",
        "outputId": "dea6bf7b-9c7f-4da8-83e1-00dc75a9a09f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/kohya-trainer\n",
            "Stored 'accelerate_config' (str)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.4/543.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for elfinder-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 lz4\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 liblz4-tool lz4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 1,605 kB of archives.\n",
            "After this operation, 5,689 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.2 [45.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 lz4 amd64 1.9.3-2build2 [90.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblz4-tool all 1.9.3-2build2 [2,342 B]\n",
            "Fetched 1,605 kB in 2s (1,048 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 120882 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package lz4.\n",
            "Preparing to unpack .../lz4_1.9.3-2build2_amd64.deb ...\n",
            "Unpacking lz4 (1.9.3-2build2) ...\n",
            "Selecting previously unselected package liblz4-tool.\n",
            "Preparing to unpack .../liblz4-tool_1.9.3-2build2_all.deb ...\n",
            "Unpacking liblz4-tool (1.9.3-2build2) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Setting up lz4 (1.9.3-2build2) ...\n",
            "Setting up liblz4-tool (1.9.3-2build2) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.8/599.8 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lycoris-lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "%store accelerate_config\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "\n",
        "def install_dependencies():\n",
        "    !pip -q install --upgrade gallery-dl gdown imjoy-elfinder\n",
        "    !apt -q install liblz4-tool aria2\n",
        "    !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip -q install -U -I --no-deps xformers==0.0.17rc481\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt9EJv5gQXuB"
      },
      "source": [
        "## 1.3. Sign-in to Cloud Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl2zERHbBQ9W",
        "outputId": "c877d1fc-f751-4d42-b876-8d676f883ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Stored 'write_token' (str)\n"
          ]
        }
      ],
      "source": [
        "#@title ### 1.3.1. Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "#@markdown Login to Huggingface hub\n",
        "#@markdown 1. You need a Huggingface account.\n",
        "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
        "write_token = \"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\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKL38-WmQsLN",
        "outputId": "3e4c46f7-bf2c-4d8f-8693-05c561408673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title ### 1.3.2. Mount Drive (Optional)\n",
        "from google.colab import drive\n",
        "\n",
        "mount_drive = True #@param {type: \"boolean\"}\n",
        "\n",
        "if mount_drive:\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gob9_OwTlwh"
      },
      "source": [
        "# II. Pretrained Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3. Download Available VAE (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoucgZQ6jgPQ",
        "outputId": "6f48fb5d-5c37-4b25-fb30-f08395c7ecae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "66f760|\u001b[1;32mOK\u001b[0m  |   228MiB/s|/content/vae/stablediffusion.vae.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installVae = []\n",
        "#@markdown ### Available VAE\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"stablediffusion.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Q23fUEJhnC",
        "outputId": "a65403f8-74c5-4654-b1b0-3a40f50158e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'train_folder_directory' (str)\n",
            "Stored 'reg_folder_directory' (str)\n"
          ]
        }
      ],
      "source": [
        "#@title ## 5.1. Define Important folder\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "project_name = \"astana_arch\" #@param {type:\"string\"}\n",
        "pretrained_model_name_or_path = \"/content/drive/MyDrive/training/pre_trained/architectureExterior_v60.safetensors\" #@param {type:\"string\"}\n",
        "vae = \"/content/vae/stablediffusion.vae.pt\"  #@param {type:\"string\"}\n",
        "#@markdown You need to register parent folder and not where `train_data_dir` located\n",
        "train_folder_directory = \"/content/drive/MyDrive/training/img/img\" #@param {'type':'string'}\n",
        "%store train_folder_directory\n",
        "reg_folder_directory = \"/content/drive/MyDrive/training/reg\" #@param {'type':'string'}\n",
        "%store reg_folder_directory\n",
        "output_dir = \"/content/drive/MyDrive/training/models\" #@param {'type':'string'}\n",
        "resume_path =\"\"\n",
        "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/fine_tune/output` by default\n",
        "output_to_drive = False #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/dreambooth/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if v2 and not v_parameterization:\n",
        "  inference_url += \"v2-inference.yaml\"\n",
        "if v2 and v_parameterization:\n",
        "  inference_url += \"v2-inference-v.yaml\"\n",
        "\n",
        "try:\n",
        "  if v2:\n",
        "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
        "    print(\"File successfully downloaded\")\n",
        "except:\n",
        "  print(\"There was an error downloading the file. Please check the URL and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2. Define Specific LoRA Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P-QVvHMUrFB",
        "outputId": "159e320f-306c-4227-a886-91ccb46956c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading network module: networks.lora\n",
            "networks.lora dim set to: 128\n",
            "networks.lora alpha set to: 128\n",
            "No LoRA weight loaded.\n",
            "Global learning rate:  0.0001\n",
            "Enabling LoRA for U-Net\n",
            "Enabling LoRA for Text Encoder\n",
            "Text encoder learning rate:  5e-05\n",
            "Learning rate Scheduler: cosine\n",
            "Training comment: this_comment_will_be_stored_in_the_metadata\n"
          ]
        }
      ],
      "source": [
        "%store -r\n",
        "\n",
        "#@markdown ## LoRA - Low Rank Adaptation Dreambooth\n",
        "\n",
        "#@markdown Some people recommend setting the `network_dim` to a higher value.\n",
        "network_dim = 128 #@param {'type':'number'}\n",
        "#@markdown For weight scaling in LoRA, it is better to set `network_alpha` the same as `network_dim` unless you know what you're doing. A lower `network_alpha` requires a higher learning rate. For example, if `network_alpha = 1`, then `unet_lr = 1e-3`.\n",
        "network_alpha = 128 #@param {'type':'number'}\n",
        "network_module = \"networks.lora\"\n",
        "\n",
        "#@markdown `network_weights` can be specified to resume training.\n",
        "network_weights = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown By default, both Text Encoder and U-Net LoRA modules are enabled. Use `network_train_on` to specify which module to train.\n",
        "network_train_on = \"both\" #@param ['both','unet_only', 'text_encoder_only'] {'type':'string'}\n",
        "\n",
        "#@markdown It is recommended to set the `text_encoder_lr` to a lower learning rate, such as `5e-5`, or to set `text_encoder_lr = 1/2 * unet_lr`.\n",
        "learning_rate = 1e-4 #@param {'type':'number'}\n",
        "unet_lr = 0 #@param {'type':'number'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"cosine\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "\n",
        "#@markdown If `lr_scheduler = cosine_with_restarts`, update `lr_scheduler_num_cycles`.\n",
        "lr_scheduler_num_cycles = 1 #@param {'type':'number'}\n",
        "#@markdown If `lr_scheduler = polynomial`, update `lr_scheduler_power`.\n",
        "lr_scheduler_power = 1 #@param {'type':'number'}\n",
        "\n",
        "#@markdown Check the box to not save metadata in the output model.\n",
        "no_metadata = False #@param {type:\"boolean\"}\n",
        "training_comment = \"this comment will be stored in the metadata\" #@param {'type':'string'}\n",
        "\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if network_weights == \"\":\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weights):\n",
        "    print(\"Loading LoRA weight:\", network_weights)\n",
        "  else:\n",
        "    print(f\"{network_weights} does not exist.\")\n",
        "    network_weights =\"\"\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enabling LoRA for U-Net.\")\n",
        "  print(\"Disabling LoRA for Text Encoder.\")\n",
        "\n",
        "print(\"Global learning rate: \", learning_rate)\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enable LoRA for U-Net\")\n",
        "  print(\"Disable LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
        "if network_train_on == \"text_encoder_only\":\n",
        "  print(\"Disabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
        "else:\n",
        "  print(\"Enabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
        "\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- Number of cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- Power: \", lr_scheduler_power)\n",
        "\n",
        "# Printing the training comment if metadata is not disabled and a comment is present\n",
        "if not no_metadata:\n",
        "  if training_comment:\n",
        "    training_comment = training_comment.replace(\" \", \"_\")\n",
        "    print(\"Training comment:\", training_comment)\n",
        "else:\n",
        "  print(\"Metadata won't be saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qWTguXmczlYo",
        "outputId": "2fbce1fd-1b5f-4117-f26a-f7709ec012d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.17rc481)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Collecting pyre-extensions==0.0.23 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n",
            "Collecting torch==2.0.0 (from xformers)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect (from pyre-extensions==0.0.23->xformers)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.23->xformers) (4.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->xformers)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0->xformers)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->xformers) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (3.27.7)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->xformers)\n",
            "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.23->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=04228ff184e1606ec41b2312dd53e71d8c8725086221a4c3cfa7b7a5044a9991\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, typing-inspect, nvidia-cusolver-cu11, nvidia-cudnn-cu11, pyre-extensions, triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-17.0.6 mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.23 torch-2.0.0 triton-2.0.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17283 sha256=7157f155aaaed679fccda130407b64900531c8004e6debf59fdabb85f379ed66\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/0e/b79b08e45752491b9acfdbd69e8a609e8b2ed7640dda5a3e59\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-8.6.1.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install xformers\n",
        "!pip install tensorrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3. Start LoRA Dreambooth Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIvfcS1pOrGY",
        "outputId": "06914fd0-7950-4d8a-dec3-796b03e3ee35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/kohya-trainer\n",
            "+-------------------------------+----------------------------------------------------------------------------------+\n",
            "| Hyperparameter                | Value                                                                            |\n",
            "+-------------------------------+----------------------------------------------------------------------------------+\n",
            "| v2                            | False                                                                            |\n",
            "| v_parameterization            | False                                                                            |\n",
            "| network_dim                   | 128                                                                              |\n",
            "| network_alpha                 | 128                                                                              |\n",
            "| network_module                | networks.lora                                                                    |\n",
            "| network_weights               | False                                                                            |\n",
            "| network_train_on              | both                                                                             |\n",
            "| learning_rate                 | 0.0001                                                                           |\n",
            "| unet_lr                       | 0                                                                                |\n",
            "| text_encoder_lr               | 5e-05                                                                            |\n",
            "| no_metadata                   | False                                                                            |\n",
            "| training_comment              | this_comment_will_be_stored_in_the_metadata                                      |\n",
            "| lr_scheduler                  | cosine                                                                           |\n",
            "| lr_scheduler_num_cycles       | 1                                                                                |\n",
            "| lr_scheduler_power            | 1                                                                                |\n",
            "| pretrained_model_name_or_path | /content/drive/MyDrive/training/pre_trained/architectureExterior_v60.safetensors |\n",
            "| vae                           | /content/vae/stablediffusion.vae.pt                                              |\n",
            "| caption_extension             | .txt                                                                             |\n",
            "| train_folder_directory        | /content/drive/MyDrive/training/img/img                                          |\n",
            "| reg_folder_directory          | /content/drive/MyDrive/training/reg                                              |\n",
            "| output_dir                    | /content/drive/MyDrive/training/models                                           |\n",
            "| prior_loss_weight             | 1.0                                                                              |\n",
            "| resume_path                   | False                                                                            |\n",
            "| project_name                  | astana_arch                                                                      |\n",
            "| mixed_precision               | fp16                                                                             |\n",
            "| save_precision                | fp16                                                                             |\n",
            "| save_n_epochs_type            | save_every_n_epochs                                                              |\n",
            "| save_n_epochs_type_value      | 1                                                                                |\n",
            "| save_model_as                 | safetensors                                                                      |\n",
            "| resolution                    | 512                                                                              |\n",
            "| enable_bucket                 | True                                                                             |\n",
            "| min_bucket_reso               | 256                                                                              |\n",
            "| max_bucket_reso               | 1024                                                                             |\n",
            "| cache_latents                 | True                                                                             |\n",
            "| train_batch_size              | 6                                                                                |\n",
            "| max_token_length              | 225                                                                              |\n",
            "| use_8bit_adam                 | True                                                                             |\n",
            "| num_epochs                    | 18                                                                               |\n",
            "| seed                          | 0                                                                                |\n",
            "| gradient_checkpointing        | False                                                                            |\n",
            "| gradient_accumulation_steps   | 1                                                                                |\n",
            "| clip_skip                     | 2                                                                                |\n",
            "| logging_dir                   | /content/dreambooth/logs                                                         |\n",
            "| log_prefix                    | astana_arch                                                                      |\n",
            "| additional_argument           | --shuffle_caption --xformers                                                     |\n",
            "+-------------------------------+----------------------------------------------------------------------------------+\n",
            "2023-12-02 10:34:32.131676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-02 10:34:38.329000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "CUDA backend failed to initialize: Found CUDA version 11070, but JAX was built against version 11080, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "prepare tokenizer\n",
            "Downloading vocab.json: 100% 961k/961k [00:00<00:00, 6.08MB/s]\n",
            "Downloading merges.txt: 100% 525k/525k [00:00<00:00, 11.0MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 2.51MB/s]\n",
            "Downloading tokenizer_config.json: 100% 905/905 [00:00<00:00, 5.31MB/s]\n",
            "update token length: 225\n",
            "Use DreamBooth method.\n",
            "prepare images.\n",
            "found directory /content/drive/MyDrive/training/img/img/7_nu contains 36 image files\n",
            "found directory /content/drive/MyDrive/training/img/img/8_tvorchestvo contains 35 image files\n",
            "found directory /content/drive/MyDrive/training/img/img/8_hanshatyr contains 34 image files\n",
            "found directory /content/drive/MyDrive/training/img/img/8_dvoresM contains 33 image files\n",
            "found directory /content/drive/MyDrive/training/img/img/8_akorda contains 31 image files\n",
            "found directory /content/drive/MyDrive/training/img/img/6_baiterek contains 36 image files\n",
            "found directory /content/drive/MyDrive/training/img/img/8_indpnd contains 23 image files\n",
            "found directory /content/drive/MyDrive/training/reg/1_reg contains 235 image files\n",
            "1716 train images with repeating.\n",
            "235 reg images.\n",
            "[Dataset 0]\n",
            "  batch_size: 6\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/7_nu\"\n",
            "    image_count: 36\n",
            "    num_repeats: 7\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: nu\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 1 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/8_tvorchestvo\"\n",
            "    image_count: 35\n",
            "    num_repeats: 8\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: tvorchestvo\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 2 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/8_hanshatyr\"\n",
            "    image_count: 34\n",
            "    num_repeats: 8\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: hanshatyr\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 3 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/8_dvoresM\"\n",
            "    image_count: 33\n",
            "    num_repeats: 8\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: dvoresM\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 4 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/8_akorda\"\n",
            "    image_count: 31\n",
            "    num_repeats: 8\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: akorda\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 5 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/6_baiterek\"\n",
            "    image_count: 36\n",
            "    num_repeats: 6\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: baiterek\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 6 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/img/img/8_indpnd\"\n",
            "    image_count: 23\n",
            "    num_repeats: 8\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: False\n",
            "    class_tokens: indpnd\n",
            "    caption_extension: .txt\n",
            "\n",
            "  [Subset 7 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/training/reg/1_reg\"\n",
            "    image_count: 235\n",
            "    num_repeats: 1\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    is_reg: True\n",
            "    class_tokens: reg\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 463/463 [01:18<00:00,  5.88it/s] \n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (384, 640), count: 98\n",
            "bucket 1: resolution (448, 576), count: 115\n",
            "bucket 2: resolution (512, 512), count: 118\n",
            "bucket 3: resolution (576, 448), count: 731\n",
            "bucket 4: resolution (640, 384), count: 2197\n",
            "bucket 5: resolution (704, 320), count: 98\n",
            "bucket 6: resolution (768, 320), count: 16\n",
            "bucket 7: resolution (832, 256), count: 44\n",
            "bucket 8: resolution (896, 256), count: 15\n",
            "mean ar error (without repeats): 0.11468032290604326\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading config.json: 100% 4.52k/4.52k [00:00<00:00, 23.1MB/s]\n",
            "Downloading model.safetensors: 100% 1.71G/1.71G [00:17<00:00, 95.8MB/s]\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: /content/vae/stablediffusion.vae.pt\n",
            "additional VAE loaded\n",
            "Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "100% 463/463 [01:15<00:00,  6.17it/s]\n",
            "import network module: networks.lora\n",
            "create LoRA network. base dim (rank): 128, alpha: 128.0\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 18 epochs is / 指定エポックまでのステップ数: 10386\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 1716\n",
            "  num reg images / 正則化画像の数: 235\n",
            "  num batches per epoch / 1epochのバッチ数: 577\n",
            "  num epochs / epoch数: 18\n",
            "  batch size per device / バッチサイズ: 6\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 10386\n",
            "steps:   0% 0/10386 [00:00<?, ?it/s]epoch 1/18\n",
            "steps:   6% 577/10386 [13:30<3:49:43,  1.41s/it, loss=0.139]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000001.safetensors\n",
            "epoch 2/18\n",
            "steps:  11% 1154/10386 [27:03<3:36:31,  1.41s/it, loss=0.139]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000002.safetensors\n",
            "epoch 3/18\n",
            "steps:  17% 1731/10386 [40:37<3:23:08,  1.41s/it, loss=0.134]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000003.safetensors\n",
            "epoch 4/18\n",
            "steps:  22% 2308/10386 [54:10<3:09:36,  1.41s/it, loss=0.131]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000004.safetensors\n",
            "epoch 5/18\n",
            "steps:  28% 2885/10386 [1:07:44<2:56:07,  1.41s/it, loss=0.134]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000005.safetensors\n",
            "epoch 6/18\n",
            "steps:  33% 3462/10386 [1:21:17<2:42:34,  1.41s/it, loss=0.126]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000006.safetensors\n",
            "epoch 7/18\n",
            "steps:  39% 4039/10386 [1:34:51<2:29:04,  1.41s/it, loss=0.125]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000007.safetensors\n",
            "epoch 8/18\n",
            "steps:  44% 4616/10386 [1:48:26<2:15:33,  1.41s/it, loss=0.122]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000008.safetensors\n",
            "epoch 9/18\n",
            "steps:  50% 5193/10386 [2:02:01<2:02:01,  1.41s/it, loss=0.12] saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000009.safetensors\n",
            "epoch 10/18\n",
            "steps:  56% 5770/10386 [2:15:35<1:48:28,  1.41s/it, loss=0.121]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000010.safetensors\n",
            "epoch 11/18\n",
            "steps:  61% 6347/10386 [2:29:10<1:34:55,  1.41s/it, loss=0.116]saving checkpoint: /content/drive/MyDrive/training/models/astana_arch-000011.safetensors\n",
            "epoch 12/18\n",
            "steps:  63% 6533/10386 [2:33:35<1:30:35,  1.41s/it, loss=0.117]"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "import textwrap\n",
        "import yaml\n",
        "\n",
        "%store -r\n",
        "\n",
        "train_batch_size = 6 #@param {type:\"number\"}\n",
        "num_epochs = 18 #@param {type:\"number\"}\n",
        "caption_extension = '.txt' #@param {'type':'string'}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_every_n_epochs\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "enable_bucket = True #@param {type:\"boolean\"}\n",
        "min_bucket_reso = 320 if resolution > 640 else 256\n",
        "max_bucket_reso = 1280 if resolution > 640 else 1024\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "use_8bit_adam = True #@param {type:\"boolean\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = 0 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/dreambooth/logs\"\n",
        "log_prefix = project_name\n",
        "additional_argument = \"--shuffle_caption --xformers\" #@param {type:\"string\"}\n",
        "print_hyperparameter = True #@param {type:\"boolean\"}\n",
        "prior_loss_weight = 1.0\n",
        "%cd {repo_dir}\n",
        "\n",
        "train_command=f\"\"\"\n",
        "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_dim={network_dim} \\\n",
        "  --network_alpha={network_alpha} \\\n",
        "  --network_module={network_module} \\\n",
        "  {\"--network_weights=\" + network_weights if network_weights else \"\"} \\\n",
        "  {\"--network_train_unet_only\" if network_train_on == \"unet_only\" else \"\"} \\\n",
        "  {\"--network_train_text_encoder_only\" if network_train_on == \"text_encoder_only\" else \"\"} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  {\"--unet_lr=\" + format(unet_lr) if unet_lr !=0 else \"\"} \\\n",
        "  {\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr !=0 else \"\"} \\\n",
        "  {\"--no_metadata\" if no_metadata else \"\"} \\\n",
        "  {\"--training_comment=\" + training_comment if training_comment and not no_metadata else \"\"} \\\n",
        "  --lr_scheduler={lr_scheduler} \\\n",
        "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_num_cycles) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
        "  {\"--lr_scheduler_power=\" + format(lr_scheduler_power) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
        "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  {\"--caption_extension=\" + caption_extension if caption_extension else \"\"} \\\n",
        "  --train_data_dir={train_folder_directory} \\\n",
        "  --reg_data_dir={reg_folder_directory} \\\n",
        "  --output_dir={output_dir} \\\n",
        "  --prior_loss_weight={prior_loss_weight} \\\n",
        "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
        "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --save_precision={save_precision} \\\n",
        "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
        "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
        "  --save_model_as={save_model_as} \\\n",
        "  --resolution={resolution} \\\n",
        "  {\"--enable_bucket\" if enable_bucket else \"\"} \\\n",
        "  {\"--min_bucket_reso=\" + format(min_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--max_bucket_reso=\" + format(max_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--cache_latents\" if cache_latents else \"\"} \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  --max_token_length={max_token_length} \\\n",
        "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
        "  --max_train_epochs={num_epochs} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
        "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --logging_dir={logging_dir} \\\n",
        "  --log_prefix={log_prefix} \\\n",
        "  {additional_argument}\n",
        "  \"\"\"\n",
        "\n",
        "debug_params = [\"v2\", \\\n",
        "                \"v_parameterization\", \\\n",
        "                \"network_dim\", \\\n",
        "                \"network_alpha\", \\\n",
        "                \"network_module\", \\\n",
        "                \"network_weights\", \\\n",
        "                \"network_train_on\", \\\n",
        "                \"learning_rate\", \\\n",
        "                \"unet_lr\", \\\n",
        "                \"text_encoder_lr\", \\\n",
        "                \"no_metadata\", \\\n",
        "                \"training_comment\", \\\n",
        "                \"lr_scheduler\", \\\n",
        "                \"lr_scheduler_num_cycles\", \\\n",
        "                \"lr_scheduler_power\", \\\n",
        "                \"pretrained_model_name_or_path\", \\\n",
        "                \"vae\", \\\n",
        "                \"caption_extension\", \\\n",
        "                \"train_folder_directory\", \\\n",
        "                \"reg_folder_directory\", \\\n",
        "                \"output_dir\", \\\n",
        "                \"prior_loss_weight\", \\\n",
        "                \"resume_path\", \\\n",
        "                \"project_name\", \\\n",
        "                \"mixed_precision\", \\\n",
        "                \"save_precision\", \\\n",
        "                \"save_n_epochs_type\", \\\n",
        "                \"save_n_epochs_type_value\", \\\n",
        "                \"save_model_as\", \\\n",
        "                \"resolution\", \\\n",
        "                \"enable_bucket\", \\\n",
        "                \"min_bucket_reso\", \\\n",
        "                \"max_bucket_reso\", \\\n",
        "                \"cache_latents\", \\\n",
        "                \"train_batch_size\", \\\n",
        "                \"max_token_length\", \\\n",
        "                \"use_8bit_adam\", \\\n",
        "                \"num_epochs\", \\\n",
        "                \"seed\", \\\n",
        "                \"gradient_checkpointing\", \\\n",
        "                \"gradient_accumulation_steps\", \\\n",
        "                \"clip_skip\", \\\n",
        "                \"logging_dir\", \\\n",
        "                \"log_prefix\", \\\n",
        "                \"additional_argument\"]\n",
        "\n",
        "if print_hyperparameter:\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
        "    for params in debug_params:\n",
        "        if params != \"\":\n",
        "            if globals()[params] == \"\":\n",
        "                value = \"False\"\n",
        "            else:\n",
        "                value = globals()[params]\n",
        "            table.add_row([params, value])\n",
        "    table.align = \"l\"\n",
        "    print(table)\n",
        "\n",
        "    arg_list = train_command.split()\n",
        "    mod_train_command = {'command': arg_list}\n",
        "\n",
        "    train_folder = os.path.dirname(output_dir)\n",
        "\n",
        "    # save the YAML string to a file\n",
        "    with open(str(train_folder)+'/dreambooth_lora_cmd.yaml', 'w') as f:\n",
        "        yaml.dump(mod_train_command, f)\n",
        "\n",
        "f = open(\"./train.sh\", \"w\")\n",
        "f.write(train_command)\n",
        "f.close()\n",
        "!chmod +x ./train.sh\n",
        "!./train.sh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
